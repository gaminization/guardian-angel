{"version":3,"file":"background.js","mappings":";;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://guardian-angel/./src/background/background.js"],"sourcesContent":["try {\r\n    // Initialize the local LLM service\r\n    let llmServiceReady = false;\r\n    let llmServiceUrl = 'http://localhost:8000/api/analyze';\r\n  \r\n    // Queue for batching requests\r\n    let analysisQueue = [];\r\n    let processingQueue = false;\r\n    const BATCH_SIZE = 4;\r\n    const BATCH_DELAY = 500; // ms\r\n  \r\n    // Process the queue in batches\r\n    async function processQueue() {\r\n      if (processingQueue || analysisQueue.length === 0) return;\r\n      \r\n      processingQueue = true;\r\n      \r\n      try {\r\n        // Take up to BATCH_SIZE items from the queue\r\n        const batch = analysisQueue.splice(0, BATCH_SIZE);\r\n        const batchPromises = batch.map(item => \r\n          analyzeWithLocalLLM(item.text, item.sensitivity)\r\n            .then(result => {\r\n              if (result.isHarassment) {\r\n                // Update detection count\r\n                chrome.storage.local.get(['detectionCount'], (data) => {\r\n                  const newCount = (data.detectionCount || 0) + 1;\r\n                  chrome.storage.local.set({ detectionCount: newCount });\r\n                });\r\n                \r\n                // Send alert to content script\r\n                chrome.tabs.sendMessage(item.tabId, {\r\n                  action: 'harassmentDetected',\r\n                  text: result.text,\r\n                  confidence: result.confidence\r\n                });\r\n              }\r\n              return result;\r\n            })\r\n        );\r\n        \r\n        await Promise.all(batchPromises);\r\n        \r\n        // Process next batch if there are items left\r\n        if (analysisQueue.length > 0) {\r\n          setTimeout(processQueue, 0);\r\n        }\r\n      } catch (error) {\r\n        console.error('Error processing batch:', error);\r\n      } finally {\r\n        processingQueue = false;\r\n      }\r\n    }\r\n  \r\n    // Function to analyze text using a local LLM service\r\n    async function analyzeWithLocalLLM(text, sensitivity) {\r\n      try {\r\n        const controller = new AbortController();\r\n        const timeoutId = setTimeout(() => controller.abort(), 5000); // 5 second timeout\r\n        \r\n        const response = await fetch(llmServiceUrl, {\r\n          method: 'POST',\r\n          headers: {\r\n            'Content-Type': 'application/json'\r\n          },\r\n          body: JSON.stringify({\r\n            text: text,\r\n            sensitivity: sensitivity\r\n          }),\r\n          signal: controller.signal,\r\n          mode: 'cors' // Explicitly set CORS mode\r\n        });\r\n        \r\n        clearTimeout(timeoutId);\r\n        \r\n        if (!response.ok) {\r\n          throw new Error(`HTTP error! status: ${response.status}`);\r\n        }\r\n        \r\n        const data = await response.json();\r\n        llmServiceReady = true; // Mark service as ready if successful\r\n        return {\r\n          isHarassment: data.isHarassment,\r\n          confidence: data.confidence || 0.5,\r\n          text: text\r\n        };\r\n      } catch (error) {\r\n        console.error('Error analyzing text with local LLM:', error.name, error.message);\r\n        // If service is unavailable, mark it as not ready\r\n        if (error.name === 'AbortError' || error.message.includes('Failed to fetch')) {\r\n          llmServiceReady = false;\r\n        }\r\n        // Fallback to simple keyword detection\r\n        return analyzeWithKeywords(text, sensitivity);\r\n      }\r\n    }\r\n  \r\n    // Simple fallback keyword-based analysis\r\n    function analyzeWithKeywords(text, sensitivity) {\r\n        const harassmentKeywords = {\r\n            high: [\r\n              'bitch', 'slut', 'whore', 'cunt', 'skank', \r\n              'get back to the kitchen', 'make me a sandwich',\r\n              'asking for it', 'should be raped', 'kill yourself', \r\n              'sleep with me', 'sleep with him', 'show me your', \r\n              'send nudes', 'send me pics', 'your body is'\r\n            ],\r\n            medium: [\r\n              'dumb girl', 'stupid woman', 'females are', 'like a girl',\r\n              'for a woman', 'emotional', 'hysteric', 'attention seeking',\r\n              'women can\\'t', 'women shouldn\\'t', 'women belong', \r\n              'on her period', 'pms-ing', 'too sensitive', \r\n              'playing the victim', 'playing victim', 'drama queen'\r\n            ],\r\n            low: [\r\n              'bossy', 'shrill', 'nagging', 'feminazi', 'man-hater',\r\n              'too emotional', 'calm down', 'smile more', \r\n              'not like other girls', 'high maintenance', 'friendzone',\r\n              'asking for attention', 'fishing for compliments'\r\n            ]\r\n          };\r\n      \r\n      // Normalize text for comparison\r\n      const normalizedText = text.toLowerCase();\r\n      \r\n      // Check for keywords based on sensitivity\r\n      let keywordsToCheck = [];\r\n      if (sensitivity === 'high') {\r\n        keywordsToCheck = harassmentKeywords.high;\r\n      } else if (sensitivity === 'medium') {\r\n        keywordsToCheck = [...harassmentKeywords.high, ...harassmentKeywords.medium];\r\n      } else {\r\n        keywordsToCheck = [...harassmentKeywords.high, ...harassmentKeywords.medium, ...harassmentKeywords.low];\r\n      }\r\n      \r\n      // Check if any keywords are present\r\n      for (const keyword of keywordsToCheck) {\r\n        if (normalizedText.includes(keyword)) {\r\n          return {\r\n            isHarassment: true,\r\n            confidence: 0.7,\r\n            text: text\r\n          };\r\n        }\r\n      }\r\n      \r\n      return {\r\n        isHarassment: false,\r\n        confidence: 0.3,\r\n        text: text\r\n      };\r\n    }\r\n  \r\n    // Listen for messages from content script\r\n    chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {\r\n      if (message.action === 'analyzeText') {\r\n        // Add to queue with tab ID\r\n        analysisQueue.push({\r\n          text: message.text,\r\n          sensitivity: message.sensitivity,\r\n          tabId: sender.tab.id\r\n        });\r\n        \r\n        // Start processing if not already running\r\n        if (!processingQueue) {\r\n          setTimeout(processQueue, BATCH_DELAY);\r\n        }\r\n      }\r\n      \r\n      // Must return true for asynchronous response\r\n      return true;\r\n    });\r\n  \r\n    // Set default values when extension is installed\r\n    chrome.runtime.onInstalled.addListener(() => {\r\n      chrome.storage.local.set({\r\n        enabled: true,\r\n        sensitivity: 'medium',\r\n        detectionCount: 0\r\n      });\r\n      console.log('Harassment Shield extension installed successfully');\r\n    });\r\n  } catch (error) {\r\n    console.error('Background script initialization error:', error);\r\n  }\r\n  \r\n\r\n"],"names":[],"sourceRoot":""}