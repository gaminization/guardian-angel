{"version":3,"file":"background.js","mappings":";;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://guardian-angel/./src/background/background.js"],"sourcesContent":["try{\r\n\r\n// Initialize the local LLM service\r\nlet llmServiceReady = false;\r\nlet llmServiceUrl = 'http://localhost:8000/api/analyze';\r\n\r\n// Function to analyze text using a local LLM service\r\n// Function to analyze text using a local LLM service\r\nasync function analyzeWithLocalLLM(text, sensitivity) {\r\n    try {\r\n      // Check if service is available\r\n      const controller = new AbortController();\r\n      const timeoutId = setTimeout(() => controller.abort(), 5000); // 5 second timeout\r\n      \r\n      const response = await fetch(llmServiceUrl, {\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json'\r\n        },\r\n        body: JSON.stringify({\r\n          text: text,\r\n          sensitivity: sensitivity\r\n        }),\r\n        signal: controller.signal\r\n      });\r\n      \r\n      clearTimeout(timeoutId);\r\n      \r\n      if (!response.ok) {\r\n        throw new Error(`HTTP error! status: ${response.status}`);\r\n      }\r\n      \r\n      const data = await response.json();\r\n      llmServiceReady = true; // Mark service as ready if successful\r\n      return {\r\n        isHarassment: data.isHarassment,\r\n        confidence: data.confidence || 0.5,\r\n        text: text\r\n      };\r\n    } catch (error) {\r\n      console.error('Error analyzing text with local LLM:', error);\r\n      // If service is unavailable, mark it as not ready\r\n      if (error.name === 'AbortError' || error.message.includes('Failed to fetch')) {\r\n        llmServiceReady = false;\r\n      }\r\n      // Fallback to simple keyword detection\r\n      return analyzeWithKeywords(text, sensitivity);\r\n    }\r\n  }\r\n  \r\n\r\n// Simple fallback keyword-based analysis\r\nfunction analyzeWithKeywords(text, sensitivity) {\r\n  const harassmentKeywords = {\r\n    high: [\r\n      'bitch', 'slut', 'whore', 'cunt', 'skank',\r\n      'get back to the kitchen', 'make me a sandwich',\r\n      'asking for it', 'should be raped'\r\n    ],\r\n    medium: [\r\n      'dumb girl', 'stupid woman', 'females are', 'like a girl',\r\n      'for a woman', 'emotional', 'hysteric', 'attention seeking'\r\n    ],\r\n    low: [\r\n      'bossy', 'shrill', 'nagging', 'feminazi', 'man-hater'\r\n    ]\r\n  };\r\n  \r\n  // Normalize text for comparison\r\n  const normalizedText = text.toLowerCase();\r\n  \r\n  // Check for keywords based on sensitivity\r\n  let keywordsToCheck = [];\r\n  if (sensitivity === 'high') {\r\n    keywordsToCheck = harassmentKeywords.high;\r\n  } else if (sensitivity === 'medium') {\r\n    keywordsToCheck = [...harassmentKeywords.high, ...harassmentKeywords.medium];\r\n  } else {\r\n    keywordsToCheck = [...harassmentKeywords.high, ...harassmentKeywords.medium, ...harassmentKeywords.low];\r\n  }\r\n  \r\n  // Check if any keywords are present\r\n  for (const keyword of keywordsToCheck) {\r\n    if (normalizedText.includes(keyword)) {\r\n      return {\r\n        isHarassment: true,\r\n        confidence: 0.7,\r\n        text: text\r\n      };\r\n    }\r\n  }\r\n  \r\n  return {\r\n    isHarassment: false,\r\n    confidence: 0.3,\r\n    text: text\r\n  };\r\n}\r\n\r\n// Listen for messages from content script\r\nchrome.runtime.onMessage.addListener((message, sender, sendResponse) => {\r\n  if (message.action === 'analyzeText') {\r\n    analyzeWithLocalLLM(message.text, message.sensitivity)\r\n      .then(result => {\r\n        if (result.isHarassment) {\r\n          // Update detection count\r\n          chrome.storage.local.get(['detectionCount'], (data) => {\r\n            const newCount = (data.detectionCount || 0) + 1;\r\n            chrome.storage.local.set({ detectionCount: newCount });\r\n          });\r\n          \r\n          // Send alert to content script\r\n          chrome.tabs.sendMessage(sender.tab.id, {\r\n            action: 'harassmentDetected',\r\n            text: result.text,\r\n            confidence: result.confidence\r\n          });\r\n        }\r\n      })\r\n      .catch(error => console.error('Error in harassment analysis:', error));\r\n  }\r\n  \r\n  // Must return true for asynchronous response\r\n  return true;\r\n});\r\n\r\n// Set default values when extension is installed\r\nchrome.runtime.onInstalled.addListener(() => {\r\n  chrome.storage.local.set({\r\n    enabled: true,\r\n    sensitivity: 'medium',\r\n    detectionCount: 0\r\n  });\r\n});\r\n}\r\ncatch (error) {\r\n    console.error('Background script initialization error:', error);\r\n}"],"names":[],"sourceRoot":""}